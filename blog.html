<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Blog – BiasDecoded</title>
  <link rel="stylesheet" href="style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
</head>
<body>

  <header>
    <img src="https://raw.githubusercontent.com/anahitajain112-blip/elegant-website2/main/attachment_111788921.jpg" alt="Site Logo" class="logo" />
    <h1>BiasDecoded Blog</h1>
    <nav>
      <a href="index.html">Home</a>
      <a href="blog.html">Blog</a>
      <a href="simulator.html">Simulator</a>
      <a href="about.html">About</a>
    </nav>
  </header>

  <section class="content">
    <article class="blog-post">
      <h2>Bias in AI: The Invisible Architect of Inequality</h2>
      <p>Artificial Intelligence is often seen as neutral—an algorithmic brain that makes decisions based on data. But what if the data itself is biased? What if the systems we trust to be fair are quietly reinforcing inequality?</p>
      <p>AI systems learn from historical data. If that data reflects societal biases—racism, sexism, economic disparity—then the AI will absorb and replicate those patterns. For example, hiring algorithms trained on past employee data may favor male candidates if the company historically hired more men. Facial recognition systems have been shown to misidentify people of color at significantly higher rates than white individuals, leading to real-world consequences like wrongful arrests.</p>
      <p>Bias in AI isn’t just a technical flaw—it’s an ethical crisis. It raises questions about accountability, transparency, and justice. Who decides what data is “clean”? Who audits these systems? And who is held responsible when harm occurs?</p>
      <p>To build ethical AI, we must go beyond accuracy and efficiency. We need inclusive datasets, diverse development teams, and regulatory frameworks that prioritize human rights. Bias isn’t just a bug—it’s a mirror. And it’s time we looked into it.</p>
    </article>

    <article class="blog-post">
      <h2>Designing for Fairness: Can AI Be Truly Ethical?</h2>
      <p>Ethics in AI isn’t just about avoiding harm—it’s about actively designing systems that promote fairness, dignity, and equity.</p>
      <p>Most AI systems are built with performance metrics like speed, accuracy, and scalability. But ethical design asks different questions: Who benefits from this system? Who might be excluded? What unintended consequences could arise?</p>
      <p>Consider predictive policing tools. They aim to allocate law enforcement resources efficiently, but often rely on crime data that’s shaped by decades of over-policing in marginalized communities. The result? A feedback loop that reinforces systemic injustice.</p>
      <p>Ethical AI design means embedding values into every stage of development—from data collection to model training to deployment. It means involving ethicists, sociologists, and affected communities in the design process. It also means being transparent about limitations and trade-offs.</p>
      <p>True fairness in AI isn’t a checkbox—it’s a commitment. And it starts with asking hard questions, listening to diverse voices, and being willing to change course when the answers challenge our assumptions.</p>
    </article>
  </section>

  <footer>
    <p>© 2025 BiasDecoded. All rights reserved.</p>
  </footer>

</body>
</html>
