<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bias in AI – BiasDecoded</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>

  <header>
    <img src="https://raw.githubusercontent.com/anahitajain112-blip/elegant-website2/main/attachment_111788921.jpg" alt="Site Logo" class="logo" />
    <h1>BiasDecoded Blog</h1>
    <nav>
      <a href="index.html">Home</a>
      <a href="blog.html">Blog</a>
      <a href="simulator.html">Simulator</a>
      <a href="about.html">About</a>
    </nav>
  </header>

  <main>
    <section class="content blog-post">
      <h2>Bias in AI: The Invisible Architect of Inequality</h2>
      <p>Artificial Intelligence is often seen as neutral—an algorithmic brain that makes decisions based on data. But what if the data itself is biased? What if the systems we trust to be fair are quietly reinforcing inequality?</p>
      <p>AI systems learn from historical data. If that data reflects societal biases—racism, sexism, economic disparity—then the AI will absorb and replicate those patterns. For example, hiring algorithms trained on past employee data may favor male candidates if the company historically hired more men. Facial recognition systems have been shown to misidentify people of color at significantly higher rates than white individuals, leading to real-world consequences like wrongful arrests.</p>
      <p>Bias in AI isn’t just a technical flaw—it’s an ethical crisis. It raises questions about accountability, transparency, and justice. Who decides what data is “clean”? Who audits these systems? And who is held responsible when harm occurs?</p>
      <p>We must also consider the global implications. AI systems deployed in one country may be trained on data from another, carrying cultural assumptions that don’t translate. This can lead to misinterpretations, discrimination, or exclusion in international contexts.</p>
      <p>To build ethical AI, we must go beyond accuracy and efficiency. We need inclusive datasets, diverse development teams, and regulatory frameworks that prioritize human rights. Bias isn’t just a bug—it’s a mirror. And it’s time we looked into it.</p>
      <p><a href="blog.html">← Back to Blog</a></p>
    </section>
  </main>

  <footer>
    <p>© 2025 BiasDecoded. All rights reserved.</p>
  </footer>

</body>
</html>
